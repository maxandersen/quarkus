ifdef::context[:parent-context: {context}]
[id="building-and-running-the-applications_{context}"]
= Building and Running the Applications
:context: building-and-running-the-applications

We now can build the `producer` and `aggregator` applications:

[source,subs="attributes+"]
----
./mvnw clean package -f producer/pom.xml
./mvnw clean package -f aggregator/pom.xml
----

Instead of running them directly on the host machine using the Quarkus dev mode,
we're going to package them into container images and launch them via Docker Compose.
This is done in order to demonstrate scaling the `aggregator` aggregation to multiple nodes later on.

The `Dockerfile` created by Quarkus by default needs one adjustment for the `aggregator` application in order to run the Kafka Streams pipeline.
To do so, edit the file `aggregator/src/main/docker/Dockerfile.jvm` and replace the line `FROM fabric8/java-alpine-openjdk8-jre` with `FROM fabric8/java-centos-openjdk8-jdk`.

Next create a Docker Compose file (`docker-compose.yaml`) for spinning up the two applications as well as Apache Kafka and ZooKeeper like so:

[source,yaml]
----
version: '3.5'

services:
  zookeeper:
    image: strimzi/kafka:0.11.3-kafka-2.1.0
    command: [
      "sh", "-c",
      "bin/zookeeper-server-start.sh config/zookeeper.properties"
    ]
    ports:
      - "2181:2181"
    environment:
      LOG_DIR: /tmp/logs
    networks:
      - kafkastreams-network
  kafka:
    image: strimzi/kafka:0.11.3-kafka-2.1.0
    command: [
      "sh", "-c",
      "bin/kafka-server-start.sh config/server.properties --override listeners=$${KAFKA_LISTENERS} --override advertised.listeners=$${KAFKA_ADVERTISED_LISTENERS} --override zookeeper.connect=$${KAFKA_ZOOKEEPER_CONNECT} --override num.partitions=$${KAFKA_NUM_PARTITIONS}"
    ]
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      LOG_DIR: "/tmp/logs"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_NUM_PARTITIONS: 3
    networks:
      - kafkastreams-network

  producer:
    image: quarkus-quickstarts/kafka-streams-producer:1.0
    build:
      context: producer
      dockerfile: src/main/docker/Dockerfile.${QUARKUS_MODE:-jvm}
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
    networks:
      - kafkastreams-network

  aggregator:
    image: quarkus-quickstarts/kafka-streams-aggregator:1.0
    build:
      context: aggregator
      dockerfile: src/main/docker/Dockerfile.${QUARKUS_MODE:-jvm}
    environment:
      QUARKUS_KAFKA_STREAMS_BOOTSTRAP_SERVERS: kafka:9092
    networks:
      - kafkastreams-network

networks:
  kafkastreams-network:
    name: ks
----

To launch all the containers, building the `producer` and `aggregator` container images,
run `docker-compose up --build`.

You should see log statements from the `producer` application about messages being sent to the "temperature-values" topic.

Now run an instance of the _debezium/tooling_ image, attaching to the same network all the other containers run in.
This image provides several useful tools such as _kafkacat_ and _httpie_:

[source,subs="attributes+"]
----
docker run --tty --rm -i --network ks debezium/tooling:1.0
----

Within the tooling container, run _kafkacat_ to examine the results of the streaming pipeline:

[source,subs="attributes+"]
----
kafkacat -b kafka:9092 -C -o beginning -q -t temperatures-aggregated

{"avg":34.7,"count":4,"max":49.4,"min":16.8,"stationId":9,"stationName":"Marrakesh","sum":138.8}
{"avg":15.7,"count":1,"max":15.7,"min":15.7,"stationId":2,"stationName":"Snowdonia","sum":15.7}
{"avg":12.8,"count":7,"max":25.5,"min":-13.8,"stationId":7,"stationName":"Porthsmouth","sum":89.7}
...
----

You should see new values arrive as the producer continues to emit temperature measurements,
each value on the outbound topic showing the mininum, maxium and average temperature values of the represented weather station.


ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]